project_name: "AI Usage & Data Policy Web Crawler"
description: "A Python-based solution to crawl organizational websites, detect AI usage, and extract related data usage policies, presenting the findings in a filterable web interface with AI-powered search and shareable, URL-based filters."
version: 9.0

phases:
  - phase: 1. Project Setup
    tasks:
      - task: 1.1 Initialize Project Structure
        description: "Create directories for source code, tests, data, logs, and configuration."
        status: Done # Status can be: To Do, In Progress, Done
        dependencies: []
      - task: 1.2 Create requirements.txt File
        description: "Create the requirements.txt file with all necessary libraries."
        sub_tasks:
          - "1.2.1 Create a file named `requirements.txt` in the root directory."
          - "1.2.2 Add the following libraries to the file, each on a new line: playwright, beautifulsoup4, pandas, streamlit, spacy, sqlalchemy, google-generativeai, pytest, pytest-mock."
        status: Done # Status can be: To Do, In Progress, Done
        dependencies: ["1.1"]
      - task: 1.3 Install Environment and Dependencies
        description: "Create a virtual environment and install all dependencies from the created files."
        sub_tasks:
          - "1.3.1 Create a virtual environment if needed: `python -m venv venv`."
          - "1.3.2 Install Python packages from the requirements file: `pip install -r requirements.txt`."
          - "1.3.3 Install system dependencies and browsers for Playwright: `playwright install --with-deps`."
          - "1.3.4 Download spaCy language model: `python -m spacy download en_core_web_sm`."
        status: Done # Status can be: To Do, In Progress, Done
        dependencies: ["1.2"]
      - task: 1.4 Create Configuration Files
        description: "Set up a configuration file for settings and the input URL list."
        status: Done # Status can be: To Do, In Progress, Done
        dependencies: ["1.1"]

  - phase: 2. Web Crawler Module (Playwright)
    tasks:
      - task: 2.1 Implement Authentication Handler
        description: "Handle websites that require login, starting with a manual approach."
        status: Done # Status can be: To Do, In Progress, Done
        dependencies: ["1.3", "1.4"]
      - task: 2.2 Implement Incremental Crawling & Change Detection
        description: "Crawl only new or modified pages on subsequent runs."
        status: Done # Status can be: To Do, In Progress, Done
        dependencies: ["1.3", "1.4"]
      - task: 2.3 Develop Web Crawler
        description: "Create a crawler to navigate websites and extract all page URLs and content."
        file: "src/crawler/spider.py"
        sub_tasks:
          - "2.3.1 Ensure the crawler saves every discovered URL to build a complete site map."
          - "2.3.2 Save the raw data (URL, HTML content, timestamp) to `data/crawled_data.jsonl`."
        status: Done # Status can be: To Do, In Progress, Done
        dependencies: ["2.1", "2.2"]
      - task: 2.4 Write and Pass Mocked Tests for Crawler
        description: "Use pytest-mock to test the crawler's link-finding and data-extraction logic without hitting the network."
        status: Done # Status can be: To Do, In Progress, Done
        dependencies: ["2.3"]

  - phase: 3. Content Analysis Module
    tasks:
      - task: 3.1 Implement AI & Policy Keyword Detection
        description: "Develop functions to scan page text for AI and data policy keywords."
        file: "src/analyzer/keyword_detector.py"
        status: Done # Status can be: To Do, In Progress, Done
        dependencies: ["1.3"]
      - task: 3.2 Write and Pass Unit Tests for Keyword Detection
        description: "Create unit tests for the analyzer module (`test_analyzer.py`) using mock text data as input."
        file: "tests/test_analyzer.py"
        status: Done # Status can be: To Do, In Progress, Done
        dependencies: ["3.1"]
      - task: 3.3 Create Analysis Pipeline Script
        description: "A script to process the raw crawled data and perform analysis."
        file: "src/analyzer/pipeline.py"
        status: Done # Status can be: To Do, In Progress, Done
        dependencies: ["2.3", "3.2"]

  - phase: 4. Data Aggregation & Storage
    tasks:
      - task: 4.1 Aggregate Data by Domain
        description: "Process the page-level analysis to create a final summary for each root domain."
        status: Done # Status can be: To Do, In Progress, Done
        dependencies: ["3.3"]
      - task: 4.2 Write and Pass Unit Tests for Aggregator
        description: "Create unit tests for the aggregator module (`test_aggregator.py`) using mock analysis JSON data."
        status: Done # Status can be: To Do, In Progress, Done
        dependencies: ["4.1"]
      - task: 4.3 Store Final Report
        description: "Save the aggregated data to a spreadsheet-ready CSV file."
        status: Done # Status can be: To Do, In Progress, Done
        dependencies: ["4.2"]

  - phase: 5. Frontend Display (Streamlit Multi-Page App)
    tasks:
      - task: 5.1 Structure Multi-Page Application
        description: "Set up the main Streamlit app with a sidebar for page navigation."
        status: Done # Status can be: To Do, In Progress, Done
        dependencies: ["1.3"]
      - task: 5.2 Build 'AI Policy Dashboard' Page with URL Filters
        description: "Create the primary interface for filtering and viewing data, with filter states reflected in the URL."
        status: Done # Status can be: To Do, In Progress, Done
        dependencies: ["4.3", "5.1"]
      - task: 5.3 Build 'AI Search' Page
        description: "Create a page with an AI-powered search bar to query the crawled data."
        status: Done # Status can be: To Do, In Progress, Done
        dependencies: ["2.3", "5.1"]
      - task: 5.4 Write and Pass Mocked Tests for AI Search UI
        description: "Use pytest-mock to simulate the generative AI API call to test the UI logic without making real API calls."
        status: Done # Status can be: To Do, In Progress, Done
        dependencies: ["5.3"]
      - task: 5.5 Build 'Site Map' Page
        description: "Create a page with a collapsible tree view of all crawled sites and pages."
        status: Done # Status can be: To Do, In Progress, Done
        dependencies: ["2.3", "5.1"]

  - phase: 6. Execution & Documentation
    tasks:
      - task: 6.1 Create Main Runner Script
        description: "A main entry point to orchestrate the entire pipeline."
        status: Done # Status can be: To Do, In Progress, Done
        dependencies: ["4.3"]
      - task: 6.2 Create Test Runner & Logging Script
        description: "Create a script (`run_tests.sh` or similar) that executes all pytests and appends the timestamp and pass/fail status to `logs/test_log.txt`."
        status: Done # Status can be: To Do, In Progress, Done
        dependencies: ["2.4", "3.2", "4.2", "5.4"]
      - task: 6.3 Write Documentation
        description: "Create a README.md file with setup and usage instructions, including how to run tests."
        status: Done # Status can be: To Do, In Progress, Done
        dependencies: ["6.1", "6.2"]
